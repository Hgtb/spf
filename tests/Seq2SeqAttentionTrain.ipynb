{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run on GPU server\n",
    "# import sys\n",
    "# sys.path.append(\"../StockPriceForecast/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available :  True\n",
      "Cuda num :  1\n",
      "Current cuda index :  0\n",
      "Current cuda name :  NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "print(\"Cuda is available : \", torch.cuda.is_available())\n",
    "print(\"Cuda num : \", torch.cuda.device_count())  # gpu数量\n",
    "print(\"Current cuda index : \", torch.cuda.current_device())\n",
    "print(\"Current cuda name : \", torch.cuda.get_device_name(0))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All days :  2994\n",
      "Train days :  (774, 1440, 10)\n",
      "Validation days ;  (720, 1440, 10)\n",
      "Test days :  (720, 1440, 10)\n",
      "train_dataLoader :  410\n",
      "validation_dataLoader :  356\n",
      "test_dataLoader :  356\n"
     ]
    }
   ],
   "source": [
    "from libs import *\n",
    "dataPath = \"../StockPriceForecast/dataSet/data/marketData/data.nc\"\n",
    "data_len = DataLength(dataPath)\n",
    "print(\"All days : \", data_len)\n",
    "input_days = 360\n",
    "\n",
    "predict_days = 5\n",
    "test_predict_len = 360  # 预测360天\n",
    "train_dataSet = DataSet(dataPath=dataPath, isel=[1500, data_len - input_days - test_predict_len],\n",
    "                        targetDays=predict_days,\n",
    "                        encoderDecoderParameter=\"close\",\n",
    "                        targetDataParameter=\"close\")\n",
    "validation_dataSet = DataSet(dataPath=dataPath, isel=[data_len - (input_days + 2 * test_predict_len), data_len - test_predict_len],\n",
    "                             targetDays=predict_days,\n",
    "                             encoderDecoderParameter=\"close\",\n",
    "                             targetDataParameter=\"close\")\n",
    "test_dataSet = DataSet(dataPath=dataPath, isel=[data_len - (input_days + test_predict_len), data_len],\n",
    "                       targetDays=predict_days,\n",
    "                       encoderDecoderParameter=\"close\",\n",
    "                       targetDataParameter=\"close\")\n",
    "print(\"Train days : \", train_dataSet.data.shape)\n",
    "print(\"Validation days ; \", validation_dataSet.data.shape)\n",
    "print(\"Test days : \", test_dataSet.data.shape)\n",
    "\n",
    "train_dataLoader = DataLoader(train_dataSet)\n",
    "validation_dataLoader = DataLoader(validation_dataSet)\n",
    "test_dataLoader = DataLoader(test_dataSet)\n",
    "\n",
    "print(\"train_dataLoader : \", len(train_dataLoader))\n",
    "print(\"validation_dataLoader : \", len(validation_dataLoader))\n",
    "print(\"test_dataLoader : \", len(test_dataLoader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../StockPriceForecast/Models/spf_softDTW_2_410.pth\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "models_store_path = \"../StockPriceForecast/Models/\"\n",
    "days_num = len(train_dataLoader)\n",
    "loss_function_name = \"softDTW\"\n",
    "lr = 0.0001\n",
    "num_epochs = 2\n",
    "model_name = f\"spf_{loss_function_name}_{num_epochs}_{days_num}.pth\"\n",
    "model_store_path = join(models_store_path, model_name)\n",
    "# train_dataSet = train_dataSet.isel(0, 390 + days_num -1)\n",
    "# train_dataLoader = DataLoader(train_dataSet)\n",
    "print(model_store_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 training:   0%|          | 0/410 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_outputs :  torch.Size([1, 360, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m Seq2SeqAttention(input_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1440\u001B[39m,\n\u001B[0;32m      6\u001B[0m                          hidden_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4096\u001B[39m,\n\u001B[0;32m      7\u001B[0m                          output_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1440\u001B[39m,\n\u001B[0;32m      8\u001B[0m                          num_layers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,\n\u001B[0;32m      9\u001B[0m                          dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# train_dataLoader.isel(startIndex=950, endIndex=len(train_dataLoader))\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m model, ls \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_Seq2SeqAttention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43mdataLoader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataLoader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43mloss_function_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_function_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43minit_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43muse_scheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\python\\NeuralNetwork\\StockPriceForecast\\libs\\modules\\Seq2SeqAttention.py:172\u001B[0m, in \u001B[0;36mtrain_Seq2SeqAttention\u001B[1;34m(model, device, lr, loss_function_name, num_epochs, dataLoader, init_weight, use_scheduler)\u001B[0m\n\u001B[0;32m    169\u001B[0m Y \u001B[38;5;241m=\u001B[39m Y\u001B[38;5;241m.\u001B[39munsqueeze(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# Y_hat : (dec_steps, batch_size, output_size)\u001B[39;00m\n\u001B[1;32m--> 172\u001B[0m Y_hat, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43menc_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdec_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;66;03m# Y_hat : (batch_size, dec_steps, input_size)\u001B[39;00m\n\u001B[0;32m    175\u001B[0m Y_hat \u001B[38;5;241m=\u001B[39m Y_hat\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\python\\NeuralNetwork\\StockPriceForecast\\libs\\modules\\Seq2SeqAttention.py:35\u001B[0m, in \u001B[0;36mSeq2SeqAttention.forward\u001B[1;34m(self, enc_input, dec_input, steps)\u001B[0m\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe mode of the model is evaluating, but the parameter \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msteps\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not passed in.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[1;32m---> 35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43menc_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdec_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate_module(enc_input, steps)\n",
      "File \u001B[1;32mD:\\python\\NeuralNetwork\\StockPriceForecast\\libs\\modules\\Seq2SeqAttention.py:64\u001B[0m, in \u001B[0;36mSeq2SeqAttention.train_module\u001B[1;34m(self, enc_inputs, dec_inputs)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(dec_inputs)):\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;66;03m# dec_input : (batch_size=1, steps=1, input_size)\u001B[39;00m\n\u001B[0;32m     63\u001B[0m     dec_input \u001B[38;5;241m=\u001B[39m dec_inputs[step]\u001B[38;5;241m.\u001B[39munsqueeze(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 64\u001B[0m     dec_output, decoder_attention_weights, decoder_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdec_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m     dec_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense(dec_output)\n\u001B[0;32m     67\u001B[0m     output_seq\u001B[38;5;241m.\u001B[39mappend(dec_output)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\python\\NeuralNetwork\\StockPriceForecast\\libs\\modules\\Encoder_AttentionDecoder.py:59\u001B[0m, in \u001B[0;36mSeq2SeqAttentionDecoder.forward\u001B[1;34m(self, decoder_input, decoder_state)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# context :  torch.Size([1, 1, hidden_size])\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menc_outputs : \u001B[39m\u001B[38;5;124m\"\u001B[39m, enc_outputs\u001B[38;5;241m.\u001B[39msize())\n\u001B[1;32m---> 59\u001B[0m context \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menc_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menc_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# bahdanau attention\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;66;03m# print(\"query : \", query.shape)\u001B[39;00m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# print(\"context : \", context.shape)\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;66;03m# print(\"decoder_input : \", decoder_input.shape)\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# decoder_input: torch.Size([1, 1, 2 * hidden_size])\u001B[39;00m\n\u001B[0;32m     66\u001B[0m decoder_input \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([context, decoder_input], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[1;31mTypeError\u001B[0m: forward() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "from libs.modules import *\n",
    "from libs import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "model = Seq2SeqAttention(input_size=1440,\n",
    "                         hidden_size=4096,\n",
    "                         output_size=1440,\n",
    "                         num_layers=4,\n",
    "                         dropout=0.1)\n",
    "# train_dataLoader.isel(startIndex=950, endIndex=len(train_dataLoader))\n",
    "model, ls = train_Seq2SeqAttention(model=model,\n",
    "                                   device=device,\n",
    "                                   dataLoader=train_dataLoader,\n",
    "                                   loss_function_name=loss_function_name,\n",
    "                                   lr=lr,\n",
    "                                   init_weight=True,\n",
    "                                   use_scheduler=False,\n",
    "                                   num_epochs=num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model, model_store_path)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from libs.modules import *\n",
    "from libs import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"../StockPriceForecast/\")\n",
    "# model = torch.load(\"../StockPriceForecast/Models/spf_softDTW_2_410.pth\")\n",
    "validation_dataLoader.isel(0, 1)\n",
    "predicts, targets, attention_weights = eval_Seq2SeqAttention(model=model,\n",
    "                                                    dataLoader=validation_dataLoader,\n",
    "                                                    steps=5,\n",
    "                                                    device=device)\n",
    "predicts = predicts.permute(2, 0, 1).squeeze()\n",
    "targets = targets.permute(2, 0, 1).squeeze()\n",
    "print(\"predicts : \", predicts.shape)\n",
    "print(\"targets : \", targets.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import numpy as np\n",
    "show_index = 0\n",
    "trace0 = go.Scatter(\n",
    "    # x = np.linspace(0, 1, 30),\n",
    "    y = predicts[show_index].numpy(),\n",
    "    mode = \"lines\",\n",
    "    name = \"predict\"\n",
    ")\n",
    "trace1 = go.Scatter(\n",
    "    # x = np.linspace(0, 1, 30),\n",
    "    y = targets[show_index].numpy(),\n",
    "    mode = \"lines\",\n",
    "    name = \"target\"\n",
    ")\n",
    "data = [trace0, trace1]\n",
    "py.iplot(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(ls.__len__())\n",
    "trace = dict(\n",
    "    # x = np.linspace(0, len(ls), 1),\n",
    "    y = np.array(ls),\n",
    "    mode = \"lines\",\n",
    "    name = \"loss\"\n",
    ")\n",
    "fig = go.Figure(trace)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 继续训练\n",
    "# Run on GPU server\n",
    "import sys\n",
    "sys.path.append(\"../StockPriceForecast/\")\n",
    "from libs.modules import *\n",
    "from libs import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from os.path import join\n",
    "models_store_path = \"../StockPriceForecast/Models/\"\n",
    "# train_dataLoader.isel(900, len(train_dataLoader))\n",
    "days_num = len(train_dataLoader)\n",
    "loss_function_name = \"softDTW\"\n",
    "lr = 0.0001\n",
    "num_epochs = 4\n",
    "model_name = f\"spf_{loss_function_name}_{num_epochs}_{days_num}.pth\"\n",
    "model_store_path = join(models_store_path, model_name)\n",
    "print(model_store_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = torch.load(\"../StockPriceForecast/Models/spf_softDTW_1_410.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, ls = train_Seq2SeqAttention(model=model,\n",
    "                                   device=device,\n",
    "                                   dataLoader=train_dataLoader,\n",
    "                                   loss_function_name=loss_function_name,\n",
    "                                   init_weight=False,\n",
    "                                   use_scheduler=False,\n",
    "                                   lr=lr,\n",
    "                                   num_epochs=num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_store_path = \"../StockPriceForecast/Models/spf_L1_1_410.pth\"\n",
    "torch.save(model, model_store_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from libs.modules import *\n",
    "from libs import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "# model = torch.load(\"../StockPriceForecast/Models/spf_softDTW_1_910.pth\")\n",
    "sys.path.append(\"../StockPriceForecast/\")\n",
    "validation_dataLoader.isel(0, 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validation_dataLoader.reset()\n",
    "predicts, targets, attention_weights = eval_Seq2SeqAttention(model=model,\n",
    "                                                    dataLoader=validation_dataLoader,\n",
    "                                                    steps=5,\n",
    "                                                    device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import numpy as np\n",
    "time_step = 0\n",
    "trace0 = go.Scatter(\n",
    "    # x = np.linspace(0, 1, 30),\n",
    "    y = predicts[0][time_step].numpy(),\n",
    "    mode = \"lines\",\n",
    "    name = \"predict\"\n",
    ")\n",
    "trace1 = go.Scatter(\n",
    "    # x = np.linspace(0, 1, 30),\n",
    "    y = targets[0][time_step].numpy(),\n",
    "    mode = \"lines\",\n",
    "    name = \"target\"\n",
    ")\n",
    "data = [trace0, trace1]\n",
    "py.iplot(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import numpy as np\n",
    "show_batch = 6\n",
    "show_index = 7\n",
    "predicts_ = predicts[show_batch].permute(1, 0)\n",
    "targets_ = targets[show_batch].permute(1, 0)\n",
    "print(\"predicts : \", predicts.shape)\n",
    "print(\"targets : \", targets.shape)\n",
    "print(\"predicts_ : \", predicts_.shape)\n",
    "print(\"targets_ : \", predicts_.shape)\n",
    "trace0 = go.Scatter(\n",
    "    # x = np.linspace(0, 1, 30),\n",
    "    y = predicts_[show_index].numpy(),\n",
    "    mode = \"lines\",\n",
    "    name = \"predict\"\n",
    ")\n",
    "trace1 = go.Scatter(\n",
    "    # x = np.linspace(0, 1, 30),\n",
    "    y = targets_[show_index].numpy(),\n",
    "    mode = \"lines\",\n",
    "    name = \"target\"\n",
    ")\n",
    "data = [trace0, trace1]\n",
    "py.iplot(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_index = 2\n",
    "trace0 = go.Scatter(\n",
    "    # x = np.linspace(0, 1, 30),\n",
    "    y = predicts[0][show_index].numpy(),\n",
    "    mode = \"lines\",\n",
    "    name = \"predict\"\n",
    ")\n",
    "trace1 = go.Scatter(\n",
    "    # x = np.linspace(0, 1, 30),\n",
    "    y = targets[0][show_index].numpy(),\n",
    "    mode = \"lines\",\n",
    "    name = \"target\"\n",
    ")\n",
    "data = [trace0, trace1]\n",
    "py.iplot(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(ls.__len__())\n",
    "trace = dict(\n",
    "    # x = np.linspace(0, len(ls), 1),\n",
    "    y = np.array(ls),\n",
    "    mode = \"lines\",\n",
    "    name = \"loss\"\n",
    ")\n",
    "fig = go.Figure(trace)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.rand([3, 4, 5])\n",
    "print(a.shape)\n",
    "d1, _, _ = a.shape\n",
    "print(d1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}