{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Run on GPU server\n",
    "import sys\n",
    "sys.path.append(\"../StockPriceForecast/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available :  True\n",
      "Cuda num :  1\n",
      "Current cuda index :  0\n",
      "Current cuda name :  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "print(\"Cuda is available : \", torch.cuda.is_available())\n",
    "print(\"Cuda num : \", torch.cuda.device_count())  # gpu数量\n",
    "print(\"Current cuda index : \", torch.cuda.current_device())\n",
    "print(\"Current cuda name : \", torch.cuda.get_device_name(0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All days :  2994\n",
      "Train days :  (2274, 1440, 10)\n",
      "Validation days ;  (720, 1440, 10)\n",
      "Test days :  (720, 1440, 10)\n"
     ]
    }
   ],
   "source": [
    "from libs import *\n",
    "dataPath = \"../StockPriceForecast/dataSet/data/marketData/data.nc\"\n",
    "data_len = DataLength(dataPath)\n",
    "print(\"All days : \", data_len)\n",
    "input_days = 360\n",
    "predict_days = 30\n",
    "test_predict_len = 360  # 预测360天\n",
    "train_dataSet = DataSet(dataPath=dataPath, isel=[0, data_len - input_days - test_predict_len])\n",
    "print(\"Train days : \", train_dataSet.data.shape)\n",
    "validation_dataSet = DataSet(dataPath=dataPath, isel=[data_len - (input_days + 2 * test_predict_len), data_len - test_predict_len])\n",
    "print(\"Validation days ; \", validation_dataSet.data.shape)\n",
    "test_dataSet = DataSet(dataPath=dataPath, isel=[data_len - (input_days + test_predict_len), data_len])\n",
    "print(\"Test days : \", test_dataSet.data.shape)\n",
    "\n",
    "train_dataLoader = DataLoader(train_dataSet)\n",
    "validation_dataLoader = DataLoader(validation_dataSet)\n",
    "test_dataLoader = DataLoader(test_dataSet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "days_num = 100  ########################\n",
    "train_dataSet = train_dataSet.isel(0, 390 + days_num -1)\n",
    "train_dataLoader = DataLoader(train_dataSet)\n",
    "print(len(train_dataLoader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function :  L1Loss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.31 GiB (GPU 0; 23.70 GiB total capacity; 20.00 GiB already allocated; 282.56 MiB free; 21.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_69603/746542688.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0mdevice\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cuda:0\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m model, attention_weight_seq, Loss =train_ResNetSeq2SeqAttention(model=model,\n\u001B[0m\u001B[1;32m     17\u001B[0m                     \u001B[0mdataLoader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_dataLoader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlearning_rate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.001\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m                     device=device, num_epoch=1, init_weights=True, loss_function=loss_function, pre_days=30)\n",
      "\u001B[0;32m~/../StockPriceForecast/libs/modules/ResNetSeq2SeqAttention.py\u001B[0m in \u001B[0;36mtrain_ResNetSeq2SeqAttention\u001B[0;34m(model, dataLoader, init_weights, loss_function, learning_rate, num_epoch, pre_days, device)\u001B[0m\n\u001B[1;32m    210\u001B[0m                 \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    211\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 212\u001B[0;31m             \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    213\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    305\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    306\u001B[0m                 inputs=inputs)\n\u001B[0;32m--> 307\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    152\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 154\u001B[0;31m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[1;32m    155\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 1.31 GiB (GPU 0; 23.70 GiB total capacity; 20.00 GiB already allocated; 282.56 MiB free; 21.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from libs.modules import *\n",
    "from libs import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "loss_function = \"L1\"  ####################\n",
    "lr = 0.00005  ####################\n",
    "model = ResNetSeq2SeqAttention(use_checkpoint=True,\n",
    "                                resNet_layer_nums=[3,4,6,3],\n",
    "                                output_size=1440,\n",
    "                                seq2seq_hidden_size=4096,\n",
    "                                seq2seq_num_layer=3)\n",
    "# device_ids = [0, 1]\n",
    "# model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "model.train()\n",
    "device = torch.device(\"cuda:0\")\n",
    "model, attention_weight_seq, Loss =train_ResNetSeq2SeqAttention(model=model,\n",
    "                    dataLoader=train_dataLoader, learning_rate=0.001,\n",
    "                    device=device, num_epoch=1, init_weights=True, loss_function=loss_function, pre_days=30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_path = \"../StockPriceForecast/Models/test/spf_lr\"+str(lr).split(\".\")[-1]+\"_\"+str(loss_function)+\"_\"+str(len(train_dataLoader))+\".pth\"\n",
    "print(save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model, save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model_outputs = []\n",
    "target = []\n",
    "attention_weights_seq = []\n",
    "res_output = []\n",
    "for data, tar in validation_dataLoader:\n",
    "    data = data.reshape(360, 1, 120, 120).cuda().float()\n",
    "    target = tar.detach().cpu()\n",
    "    print(data.dtype)\n",
    "    model_outputs, attention_weights_seq, _ = model(data, steps=30)\n",
    "    print(model_outputs[0])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output = model_outputs.detach().cpu()\n",
    "print(output.shape)\n",
    "output = output.permute(2, 0, 1)\n",
    "print(output.shape)\n",
    "output = output.squeeze()\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_data = target\n",
    "print(target_data.shape)\n",
    "target_data = target_data.permute(1, 0)\n",
    "print(target_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import numpy as np\n",
    "show_index = 4\n",
    "trace0 = go.Scatter(\n",
    "    # x = np.linspace(0, 1, 30),\n",
    "    y = output[show_index].numpy(),\n",
    "    mode = \"lines\",\n",
    "    name = \"predict\"\n",
    ")\n",
    "trace1 = go.Scatter(\n",
    "    # x = np.linspace(0, 1, 30),\n",
    "    y = target_data[show_index].numpy(),\n",
    "    mode = \"lines\",\n",
    "    name = \"target\"\n",
    ")\n",
    "data = [trace0, trace1]\n",
    "py.iplot(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(attention_weights_seq))\n",
    "print(attention_weights_seq[0].shape)\n",
    "attention_weights = attention_weights_seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for i in range(len(attention_weights_seq)):\n",
    "    attention_weights[i].detach().cpu()\n",
    "    attention_weights[i].require_grad=False\n",
    "print(attention_weights[0].requires_grad)\n",
    "attention_weights = np.array(attention_weights_seq)\n",
    "print(attention_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(attention_weights_seq[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}